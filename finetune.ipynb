{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 24.1.2 from /home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/pip (python 3.11)\n",
      "Looking in indexes: https://REPOSdlcoeawscommons:****@common.repositories.cloud.sap/artifactory/api/pypi/deploy.releases.pypi/simple, https://REPOSdlcoeawscommons:****@common.repositories.cloud.sap/artifactory/api/pypi/deploy-releases-hyperspace-pypi/simple, https://REPOSdlcoeawscommons:****@common.repositories.cloud.sap/artifactory/api/pypi/deploy.milestones.pypi/simple, https://pypi.org/simple\n",
      "Requirement already satisfied: hf-transfer in ./.venv/lib/python3.11/site-packages (0.1.8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "required_packages = [\n",
    "    \"sagemaker>=2.192.0\",\n",
    "    \"huggingface_hub\",\n",
    "    \"hf-transfer\",\n",
    "    \"transformers==4.33.0\",\n",
    "    \"datasets\",\n",
    "    \"wandb\",\n",
    "    \"matplotlib\",\n",
    "]\n",
    "\n",
    "# Check if the required packages are installed\n",
    "not_installed = []\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.split(\"=\")[0].split(\">\")[0])\n",
    "    except ImportError:\n",
    "        not_installed.append(package)\n",
    "\n",
    "# Install the missing packages\n",
    "if not_installed:\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", *not_installed, \"--upgrade\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['huggingface-cli', 'login', '--token', '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'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Hugging Face token from the environment variable\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Check if the token is available\n",
    "if huggingface_token is None:\n",
    "    raise ValueError(\"Hugging Face token not found. Please check your .env file.\")\n",
    "\n",
    "# Login using the Hugging Face CLI with the token\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", huggingface_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philikai/Spider-SQL-LLAMA2_train\", cache_dir='./data')\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': Value(dtype='string', id=None),\n",
       " 'query': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'schema': Value(dtype='string', id=None),\n",
       " 'primary_keys': Value(dtype='string', id=None),\n",
       " 'foreign_keys': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the instruction prompt to maximize the model performance further\n",
    "def format_spider(sample):\n",
    "    instruction_prompt = f\"\"\"Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
    "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
    "    Answer the following question with the context below: \\n{sample['question']}\"\"\"\n",
    "    instruction = f\"### Instruction\\n{instruction_prompt} \"\n",
    "    context = f\"### Context\\n{sample['schema']} | {sample['foreign_keys']} | {sample['primary_keys']}\"\n",
    "    response = f\"### Answer\\n<SQL> {sample['query']} </SQL>\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "What are the names of musicals with nominee \"Bob Fosse\"? \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | musical |  musical : musical_id (text) , name (number) , year (text) , award (number) , category (text) , nominee (text) , result (text) | actor : actor_id (text) , name (number) , musical_id (text) , character (number) , duration (text) , age (text); | [Foreign Keys]: actor : musical_id = actor : actor_id | [Primary Keys]: musical : musical_id, actor : actor_id\n",
      "\n",
      "### Answer\n",
      "<SQL> SELECT Name FROM musical WHERE Nominee  =  \"Bob Fosse\" </SQL>\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_spider(dataset[\"train\"][randrange(len(dataset[\"train\"]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"codellama/CodeLlama-7b-hf\"  # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['db_id', 'query', 'question', 'schema', 'primary_keys', 'foreign_keys'],\n",
      "    num_rows: 8659\n",
      "})\n",
      "Dataset({\n",
      "    features: ['db_id', 'query', 'question', 'schema', 'primary_keys', 'foreign_keys'],\n",
      "    num_rows: 1034\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# assign just the train dataset for testing purposes\n",
    "dataset_train = dataset[\"train\"]\n",
    "dataset_validation = dataset[\"validation\"]\n",
    "\n",
    "# remove 'text' column if it exists in the dataset_train\n",
    "if \"text\" in dataset_train.column_names:\n",
    "    dataset_train = dataset_train.remove_columns(\"text\")\n",
    "print(dataset_train)\n",
    "\n",
    "# remove 'text' column if it exists in the dataset_validation\n",
    "if \"text\" in dataset_validation.column_names:\n",
    "    dataset_validation = dataset_validation.remove_columns(\"text\")\n",
    "print(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "papers published in eccv 2016 by ali farhadi \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | scholar |  venue : venueid (text) , venuename (number) | author : authorid (text) , authorname (number) | dataset : datasetid (text) , datasetname (number) | journal : journalid (text) , journalname (number) | keyphrase : keyphraseid (text) , keyphrasename (number) | paper : paperid (text) , title (number) , venueid (text) , year (number) , numciting (text) , numcitedby (number) , journalid (text) | cite : citingpaperid (text) , citedpaperid (number) | paperDataset : paperid (text) , datasetid (number) | paperKeyphrase : paperid (text) , keyphraseid (number) | writes : paperid (text) , authorid (number); | [Foreign Keys]: paper : venueid = venue : venueid | paper : journalid = journal : journalid | cite : citingpaperid = paper : paperid | cite : citedpaperid = paper : paperid | paperkeyphrase : keyphraseid = keyphrase : keyphraseid | paperkeyphrase : paperid = paper : paperid | writes : authorid = author : authorid | writes : paperid = paper : paperid | [Primary Keys]: venue : venueid, author : authorid, dataset : datasetid, journal : journalid, keyphrase : keyphraseid, paper : paperid, cite : citingpaperid, paperdataset : datasetid, paperkeyphrase : keyphraseid, writes : paperid\n",
      "\n",
      "### Answer\n",
      "<SQL> SELECT DISTINCT t3.paperid FROM venue AS t4 JOIN paper AS t3 ON t4.venueid  =  t3.venueid JOIN writes AS t2 ON t2.paperid  =  t3.paperid JOIN author AS t1 ON t2.authorid  =  t1.authorid WHERE t1.authorname  =  \"ali farhadi\" AND t3.year  =  2016 AND t4.venuename  =  \"eccv\"; </SQL></s>\n",
      "**********************************************************************************************************************************************************************************************************************************************************\n",
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "What is the zip code for Port Chelsea? \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | student_transcripts_tracking |  Addresses : address_id (text) , line_1 (number) , line_2 (text) , line_3 (text) , city (text) , zip_postcode (text) , state_province_county (text) , country (text) , other_address_details (text) | Courses : course_id (text) , course_name (number) , course_description (text) , other_details (text) | Departments : department_id (text) , department_name (number) , department_description (text) , other_details (text) | Degree_Programs : degree_program_id (text) , department_id (number) , degree_summary_name (text) , degree_summary_description (text) , other_details (text) | Sections : section_id (text) , course_id (number) , section_name (text) , section_description (text) , other_details (text) | Semesters : semester_id (text) , semester_name (number) , semester_description (text) , other_details (text) | Students : student_id (text) , current_address_id (number) , permanent_address_id (text) , first_name (text) , middle_name (text) , last_name (text) , cell_mobile_number (text) , email_address (text) , ssn (text) , date_first_registered (text) , date_left (number) , other_student_details (text) | Student_Enrolment : student_enrolment_id (text) , degree_program_id (number) , semester_id (text) , student_id (text) , other_details (text) | Student_Enrolment_Courses : student_course_id (text) , course_id (number) , student_enrolment_id (text) | Transcripts : transcript_id (text) , transcript_date (number) , other_details (text) | Transcript_Contents : student_course_id (text) , transcript_id (number); | [Foreign Keys]: degree_programs : department_id = departments : department_id | sections : course_id = courses : course_id | students : permanent_address_id = addresses : address_id | students : current_address_id = addresses : address_id | student_enrolment : student_id = students : student_id | student_enrolment : semester_id = semesters : semester_id | student_enrolment : degree_program_id = degree_programs : degree_program_id | student_enrolment_courses : student_enrolment_id = student_enrolment : student_enrolment_id | student_enrolment_courses : course_id = courses : course_id | transcript_contents : transcript_id = transcripts : transcript_id | transcript_contents : student_course_id = student_enrolment_courses : student_course_id | [Primary Keys]: addresses : address_id, courses : course_id, departments : department_id, degree_programs : degree_program_id, sections : section_id, semesters : semester_id, students : student_id, student_enrolment : student_enrolment_id, student_enrolment_courses : student_course_id, transcripts : transcript_id\n",
      "\n",
      "### Answer\n",
      "<SQL> SELECT zip_postcode FROM Addresses WHERE city  =  'Port Chelsea' </SQL></s>\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_spider(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset_train_format_ok = dataset_train.map(\n",
    "    template_dataset, remove_columns=list(dataset_train.features)\n",
    ")\n",
    "\n",
    "dataset_train_format_ok_val = dataset_validation.map(\n",
    "    template_dataset, remove_columns=list(dataset_validation.features)\n",
    ")\n",
    "# print random sample\n",
    "print(dataset_train_format_ok[randint(0, len(dataset_train_format_ok))][\"text\"])\n",
    "print(\"*\" * 250)\n",
    "print(dataset_train_format_ok_val[randint(0, len(dataset_train_format_ok_val))][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 835, 2799, 4080, 13, 29954, 5428, 385, 1881, 1139, 29892, 671, 21120, 5877, 304, 5706, 263, 4576, 2346, 491, 23906, 697, 470, 2999, 310, 278, 1494, 6131, 29889, 29871, 13, 1678, 450, 9117, 322, 7601, 6611, 674, 367, 19056, 29889, 14350, 2346, 297, 1546, 529, 4176, 2565, 4176, 15513, 29871, 13, 1678, 673, 278, 1494, 1139, 411, 278, 3030, 2400, 29901, 29871, 13, 5328, 1784, 15883, 310, 278, 5840, 1860, 526, 9642, 1135, 29871, 29945, 29953, 1577, 29871, 13, 13, 2277, 29937, 15228, 13, 29961, 12763, 313, 5975, 29897, 313, 8768, 4638, 29901, 891, 14311, 29918, 21895, 891, 29871, 14311, 584, 14311, 29918, 333, 313, 726, 29897, 1919, 1024, 313, 4537, 29897, 1919, 11265, 313, 726, 29897, 1919, 24034, 313, 726, 29897, 1919, 23562, 29918, 262, 29918, 29890, 453, 1080, 313, 4537, 29897, 1919, 954, 29918, 3451, 2376, 12712, 313, 4537, 29897, 891, 2343, 584, 2343, 29918, 333, 313, 726, 29897, 1919, 1024, 313, 4537, 29897, 1919, 6345, 29918, 3859, 313, 726, 29897, 1919, 5046, 313, 726, 29897, 891, 10643, 584, 14311, 29918, 333, 313, 726, 29897, 1919, 2343, 29918, 333, 313, 4537, 29897, 1919, 13201, 29918, 627, 292, 313, 726, 416, 891, 518, 27755, 4813, 952, 5387, 10643, 584, 2343, 29918, 333, 353, 2343, 584, 2343, 29918, 333, 891, 10643, 584, 14311, 29918, 333, 353, 14311, 584, 14311, 29918, 333, 891, 518, 26666, 4813, 952, 5387, 14311, 584, 14311, 29918, 333, 29892, 2343, 584, 2343, 29918, 333, 29892, 10643, 584, 14311, 29918, 333, 13, 13, 2277, 29937, 673, 13, 29966, 4176, 29958, 5097, 2302, 22798, 3895, 2343, 5754, 5046, 29871, 1405, 259, 29945, 29953, 1533, 4176, 29958, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets inspect what we get from the tokenizer\n",
    "tokenizer(dataset_train_format_ok[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dataset = dataset_train_format_ok.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset_train_format_ok.features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 8659\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_format_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 8659\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of any entry: 2840\n",
      "Number of entries above 2048 tokens: 82\n"
     ]
    }
   ],
   "source": [
    "def get_max_length_and_count(dataset, max_token_length):\n",
    "    \"\"\"\n",
    "    Given a dataset, this function returns the maximum length of any of the entries and counts how many\n",
    "    of them have a length above the specified max_token_length. It also plots a histogram of the lengths.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (dict): Dictionary containing 'input_ids' as keys and lists as values.\n",
    "    - max_token_length (int): Specified max token length to compare with.\n",
    "\n",
    "    Returns:\n",
    "    - (int, int): Maximum length of any of the entries and count of entries having a length above max_token_length.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extracting all lengths\n",
    "    lengths = [len(entry) for entry in dataset[\"input_ids\"]]\n",
    "\n",
    "    # Getting the maximum length\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    # Counting how many are above the specified max_token_length\n",
    "    count_above_max_token_length = sum(\n",
    "        1 for length in lengths if length > max_token_length\n",
    "    )\n",
    "\n",
    "    # Plotting a histogram of the lengths\n",
    "    plt.hist(lengths, bins=100)\n",
    "    plt.xlabel(\"Token Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Token Lengths\")\n",
    "    plt.show()\n",
    "\n",
    "    return max_length, count_above_max_token_length\n",
    "\n",
    "\n",
    "max_length_value = 2048\n",
    "max_len, count_above = get_max_length_and_count(tok_dataset, max_length_value)\n",
    "print(f\"Maximum length of any entry: {max_len}\")\n",
    "print(f\"Number of entries above {max_length_value} tokens: {count_above}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 2264\n",
      "Total number of samples: 216\n"
     ]
    }
   ],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {\n",
    "        k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()\n",
    "    }\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {\n",
    "        k: concatenated_examples[k][batch_chunk_length:]\n",
    "        for k in concatenated_examples.keys()\n",
    "    }\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk training dataset\n",
    "lm_dataset = dataset_train_format_ok.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset_train_format_ok.features),\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "\n",
    "# tokenize and chunk validation dataset\n",
    "lm_dataset_validation = dataset_train_format_ok_val.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset_train_format_ok_val.features),\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n",
    "print(f\"Total number of samples: {len(lm_dataset_validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from huggingface_hub import login, HfFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "# COPIED FROM https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "def create_peft_model(model, gradient_checkpointing=True, bf16=True):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_kbit_training,\n",
    "    )\n",
    "    from peft.tuners.lora import LoraLayer\n",
    "\n",
    "    # prepare int-4 model for training\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model, use_gradient_checkpointing=gradient_checkpointing\n",
    "    )\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # get lora target modules\n",
    "    modules = find_all_linear_names(model)\n",
    "    print(f\"Found {len(modules)} modules to quantize: {modules}\")\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # pre-process the model by upcasting the layer norms in float 32 for\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraLayer):\n",
    "            if bf16:\n",
    "                module = module.to(torch.bfloat16)\n",
    "        if \"norm\" in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if \"lm_head\" in name or \"embed_tokens\" in name:\n",
    "            if hasattr(module, \"weight\"):\n",
    "                if bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "gradient_checkpointing = True\n",
    "bf16 = True\n",
    "lr = 5e-5\n",
    "epochs = 1\n",
    "\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Define training args\n",
    "output_dir = \"./tmp/code_llama\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    bf16=bf16,  # Use BF16 if available\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:48<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# dataset = load_from_disk(args.dataset_path)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load model from the hub with a bnb config\u001b[39;00m\n\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this is needed for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# create peft config\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m create_peft_model(\n\u001b[1;32m     21\u001b[0m     model, gradient_checkpointing\u001b[38;5;241m=\u001b[39mgradient_checkpointing, bf16\u001b[38;5;241m=\u001b[39mbf16\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/efs/llm/finetune/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/efs/llm/finetune/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3175\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3166\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3168\u001b[0m     (\n\u001b[1;32m   3169\u001b[0m         model,\n\u001b[1;32m   3170\u001b[0m         missing_keys,\n\u001b[1;32m   3171\u001b[0m         unexpected_keys,\n\u001b[1;32m   3172\u001b[0m         mismatched_keys,\n\u001b[1;32m   3173\u001b[0m         offload_index,\n\u001b[1;32m   3174\u001b[0m         error_msgs,\n\u001b[0;32m-> 3175\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3182\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3183\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3186\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3187\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3193\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3194\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/efs/llm/finetune/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3563\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m   3562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_fsdp_enabled_and_dist_rank_0():\n\u001b[0;32m-> 3563\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3579\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/efs/llm/finetune/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:710\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    708\u001b[0m             set_module_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "set_seed(seed)\n",
    "# dataset = load_from_disk(args.dataset_path)\n",
    "# load model from the hub with a bnb config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_cache=False if gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# create peft config\n",
    "model = create_peft_model(\n",
    "    model, gradient_checkpointing=gradient_checkpointing, bf16=bf16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='396' max='1132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 396/1132 53:13 < 1:39:25, 0.12 it/s, Epoch 0.35/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.277200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.312600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.264100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mlm_dataset,\n\u001b[1;32m      5\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdefault_data_collator,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/efs/llm/finetune/.venv/lib/python3.11/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/efs/llm/finetune/.venv/lib/python3.11/site-packages/transformers/trainer.py:1840\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=lm_dataset,\n",
    "#     data_collator=default_data_collator,\n",
    "# )\n",
    "\n",
    "# # Start training\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.87s/it]\n",
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "# base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "save_dir = 'tmp/code_llama/max_length_value-2048-epoch-1-bs-3-lr-0.0002'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_dir,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# from peft import PeftModel\n",
    "# model = PeftModel.from_pretrained(model, 'tmp/code_llama/max_length_value-2048-epoch-1-bs-3-lr-0.0002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the instruction prompt to maximize the model performance further\n",
    "def format_spider_val(sample):\n",
    "    instruction_prompt = f\"\"\"Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
    "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
    "    Answer the following question with the context below: \\n{sample['question']}\"\"\"\n",
    "    instruction = f\"### Instruction\\n{instruction_prompt} \"\n",
    "    context = f\"### Context\\n{sample['schema']} | {sample['foreign_keys']} | {sample['primary_keys']}\"\n",
    "    response = f\"### Answer\\n\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    response = f\"<SQL> {sample['query']} </SQL>\"\n",
    "    return prompt, response\n",
    "\n",
    "# # template dataset to add prompt to each sample\n",
    "# def template_dataset(sample):\n",
    "#     sample[\"text\"] = f\"{format_spider(sample)}{tokenizer.eos_token}\"\n",
    "#     return sample\n",
    "\n",
    "# dataset_train_format_ok_val = dataset_validation.map(\n",
    "#     template_dataset, remove_columns=list(dataset_validation.features)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "Find the names of stores whose number products is more than the average number of products. \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | employee_hire_evaluation |  employee : employee_id (text) , name (number) , age (text) , city (number) | shop : shop_id (text) , name (number) , location (text) , district (number) , number_products (text) , manager_name (number) | hiring : shop_id (text) , employee_id (number) , start_from (text) , is_full_time (number) | evaluation : employee_id (text) , year_awarded (number) , bonus (text); | [Foreign Keys]: hiring : employee_id = employee : employee_id | hiring : shop_id = shop : shop_id | evaluation : employee_id = employee : employee_id | [Primary Keys]: employee : employee_id, shop : shop_id, hiring : employee_id, evaluation : employee_id\n",
      "\n",
      "### Answer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample, response = format_spider_val(dataset_validation[randint(0, len(dataset_validation))])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SQL> SELECT name FROM shop WHERE number_products  >  (SELECT avg(number_products) FROM shop) </SQL>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/ec2-user/efs/llm/finetune/.venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(sample, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**model_input, max_new_tokens=100)[0]\n",
    "    # print(output)\n",
    "    # print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Given an input question, use sqlite syntax to generate a sql query by choosing one or multiple of the following tables. \n",
      "    The foreign and primary keys will be supplied. Write query in between <SQL></SQL>. \n",
      "    Answer the following question with the context below: \n",
      "Find the names of stores whose number products is more than the average number of products. \n",
      "\n",
      "### Context\n",
      "[Schema (values) (types)]: | employee_hire_evaluation |  employee : employee_id (text) , name (number) , age (text) , city (number) | shop : shop_id (text) , name (number) , location (text) , district (number) , number_products (text) , manager_name (number) | hiring : shop_id (text) , employee_id (number) , start_from (text) , is_full_time (number) | evaluation : employee_id (text) , year_awarded (number) , bonus (text); | [Foreign Keys]: hiring : employee_id = employee : employee_id | hiring : shop_id = shop : shop_id | evaluation : employee_id = employee : employee_id | [Primary Keys]: employee : employee_id, shop : shop_id, hiring : employee_id, evaluation : employee_id\n",
      "\n",
      "### Answer\n",
      "<SQL> SELECT Name FROM shop WHERE Number_products  >  (SELECT avg(Number_products) FROM shop) </SQL>\n"
     ]
    }
   ],
   "source": [
    "new_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "print(new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
